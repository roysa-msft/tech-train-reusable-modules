{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roysa-msft/tech-train-reusable-modules/blob/main/ERM_Ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95436180",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:21:43.843679Z",
          "start_time": "2023-10-19T14:21:43.164856Z"
        },
        "id": "95436180"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pprint import pprint"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85524253",
      "metadata": {
        "id": "85524253"
      },
      "source": [
        "### Background\n",
        "\n",
        "In this exercise we will compute an ERM hypothesis for the same prediction problem mentioned in class, but each time with different assumptions (and slightly different training set). We will see that finding an ERM hypothesis isn't always computationally easy, and experience first hand the impact of the Approximation Error (in calss we called it $appx$).\n",
        "\n",
        "You are given various tables, each of which contains, $n=10^6$ softness levels and their corresponding labels (1 = tasty, 0 = otherwise). Each table is a csv file you can load with `pd.read_csv()`.\n",
        "Recall that softness is encoded as a floating number between 0 and 1.\n",
        "\n",
        "For this exercise we will use the zero-one loss, which doesn't discriminate between types of error."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6879abe2",
      "metadata": {
        "id": "6879abe2"
      },
      "source": [
        "#### Baseline predictor\n",
        "\n",
        "A learning rule is an algorithm $A$ that is given a training set $S$ and return $h\\in\\cal{H}$ (recall that $\\cal{H}$ is implied by the algorithm).\n",
        "\n",
        "Let's start with the basics.\n",
        "Your goal is to find a learning rule $A$ that has the following properties:\n",
        "* It is applicable for **any** binary classification task, which means $\\cal{H}=\\{x\\in\\cal{X}\\mapsto\\{0,1\\}\\}$ works for any the feature space $\\cal{X}$\n",
        "* It is very simple to compute (specifically, a signle pass over the dataset is suffice)\n",
        "* The resulting $h$ always have $\\cal{L}_{S}(h)\\le\\frac12$ (with respect to the zero-one loss).\n",
        "\n",
        "We will call the corresponding hypothesis $h$ the \"baseline\" hypothesis.\n",
        "\n",
        "Implement such a learning rule, and apply it to the given dataset to compute the \"baseline\" $\\theta$.\n",
        "\n",
        "Hint: if the algorithm should work for any feature space, it means that it doesn't look at the features at all."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b3f3efa",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:22:10.735768Z",
          "start_time": "2023-10-19T14:22:10.718285Z"
        },
        "id": "2b3f3efa"
      },
      "outputs": [],
      "source": [
        "df1 = pd.read_csv('s1ex1.csv')\n",
        "df1.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2000798",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:22:11.505519Z",
          "start_time": "2023-10-19T14:22:10.944451Z"
        },
        "id": "a2000798"
      },
      "outputs": [],
      "source": [
        "def baseline_hypothesis(df):\n",
        "    \"\"\" The function should return a callback which accepts a single float and return zero or one \"\"\"\n",
        "    pass\n",
        "\n",
        "def training_error(df, h):\n",
        "    return (df['label'] != df['x'].map(h)).mean()\n",
        "\n",
        "print('Empirical loss of the baseline hypothesis is:', training_error(df1, baseline_hypothesis(df1)))\n",
        "\n",
        "real_threshold = (1 / np.e) ** 2\n",
        "print('Note that the OPTIMAL threshold is', real_threshold, \"with error:\", training_error(df1, lambda x: 1 if x >= real_threshold else 0), \"and the error is not zero because the data was generated with noise.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e35c78bf",
      "metadata": {
        "id": "e35c78bf"
      },
      "source": [
        "#### Efficient ERM requires strong prior knowledge\n",
        "\n",
        "Suppose our hypothesis space is the 1 dimensional threshold functions, as showed in class, which is parametrized by a threshold $\\theta$ and a direction $b\\in\\{{\\pm1}\\}$ .\n",
        "Find an ERM hypothesis, e.g. the best threshold $\\theta$ and direction $b$, **efficiently** for the dataset `s1ex1.csv`, by running a binary search over the interval $[0,1]$.\n",
        "\n",
        "In order to do that you'll have to figure out:\n",
        "* What is the stopping criterion for the search?\n",
        "* How to choose to which side (e.g. right or left) of the $x$ axis to \"drill in\" with the recursion?\n",
        "\n",
        "Note that there is no one \"right\" answer here.\n",
        "\n",
        "You are encouraged to manually explore the dataset, here are some suggestions:\n",
        "* Plot the label as a function of $x$: `df.sample(n=10000).plot.scatter(x='x', y='label')`\n",
        "* Display the training error as a function of the threshold, by manually selecting threshold candidates, e.g.: `np.linspace(0, 1.0)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3020675",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:22:14.892536Z",
          "start_time": "2023-10-19T14:22:12.474685Z"
        },
        "code_folding": [],
        "id": "b3020675"
      },
      "outputs": [],
      "source": [
        "def to_label_stats(df):\n",
        "    return df['label'].value_counts().reindex([0,1]).fillna(0)\n",
        "\n",
        "def threshold_split_stats(left_counts, right_counts):\n",
        "    errs_if_left_equal_zero = left_counts[1] + right_counts[0]\n",
        "    errs_if_left_equal_one = left_counts[0] + right_counts[1]\n",
        "\n",
        "    best_left_label = int(errs_if_left_equal_one < errs_if_left_equal_zero)\n",
        "\n",
        "    left = {\n",
        "        'label': best_left_label,\n",
        "        'size': left_counts[0] + left_counts[1],\n",
        "        'mistakes': left_counts[1-best_left_label]\n",
        "    }\n",
        "\n",
        "    right = {\n",
        "        'label': 1 - best_left_label,\n",
        "        'size': right_counts[0] + right_counts[1],\n",
        "        'mistakes': right_counts[best_left_label]\n",
        "    }\n",
        "\n",
        "    return left, right\n",
        "\n",
        "\n",
        "def evaluate_single_threshold(df, threshold):\n",
        "    left_mask = df['x'] < threshold\n",
        "\n",
        "    left_counts = to_label_stats(df.loc[left_mask])\n",
        "    right_counts = to_label_stats(df.loc[~left_mask])\n",
        "\n",
        "    left_segment, right_segment = threshold_split_stats(left_counts, right_counts)\n",
        "\n",
        "    return {\n",
        "        'thr': threshold,\n",
        "        'left': left_segment,\n",
        "        'right': right_segment,\n",
        "        'total_mistakes': left_segment['mistakes'] + right_segment['mistakes']\n",
        "    }\n",
        "\n",
        "def calc_stats_for_multiple_thresholds(df, thresholds=np.linspace(0, 1)):\n",
        "    thresholds = pd.Series(thresholds, name='thr')\n",
        "    mistakes = thresholds.map(lambda t: evaluate_single_threshold(df, t)['total_mistakes'])\n",
        "    loss_df = pd.DataFrame({'thr': thresholds, 'total_mistakes': mistakes})\n",
        "    return loss_df\n",
        "\n",
        "loss1 = calc_stats_for_multiple_thresholds(df1)\n",
        "loss1.plot.line(x='thr', y='total_mistakes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28a0b9ee",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:22:17.678558Z",
          "start_time": "2023-10-19T14:22:14.895732Z"
        },
        "scrolled": false,
        "id": "28a0b9ee"
      },
      "outputs": [],
      "source": [
        "def binary_search_erm(df):\n",
        "    \"\"\" The function should return a callback which accepts a single float and return zero or one \"\"\"\n",
        "    pass\n",
        "\n",
        "h_binary_search1 = binary_search_erm(df1)\n",
        "print(\n",
        "    'Real threshold:', real_threshold, '\\n',\n",
        "    'empirical error:', training_error(df1, h_binary_search1)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fb815e8",
      "metadata": {
        "id": "5fb815e8"
      },
      "source": [
        "#### Failure example\n",
        "\n",
        "Unfortunately, if we aren't careful, our algorithm might fail. To see this, run your code for the file `s2ex2.csv`.\n",
        "\n",
        "Which threshold our learning algorithm returned? Is it the right one? You can see the \"best\" threshold by plotting the error as a function of selected theta-candidates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6052bec4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:22:17.693985Z",
          "start_time": "2023-10-19T14:22:17.681557Z"
        },
        "id": "6052bec4"
      },
      "outputs": [],
      "source": [
        "# evaluate on s2ex2.csv\n",
        "df2 = pd.read_csv('s1ex2.csv')\n",
        "df2.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d39bc01d",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:22:20.158008Z",
          "start_time": "2023-10-19T14:22:17.696916Z"
        },
        "id": "d39bc01d"
      },
      "outputs": [],
      "source": [
        "loss2 = calc_stats_for_multiple_thresholds(df2)\n",
        "loss2.plot.line(x='thr', y='total_mistakes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cddaffe",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:22:23.031615Z",
          "start_time": "2023-10-19T14:22:20.161008Z"
        },
        "id": "2cddaffe"
      },
      "outputs": [],
      "source": [
        "h_binary_search2 = binary_search_erm(df2)\n",
        "print(\n",
        "    'Real threshold:', real_threshold, '\\n',\n",
        "    'empirical error:', training_error(df2, h_binary_search2)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4083e755",
      "metadata": {
        "id": "4083e755"
      },
      "source": [
        "#### Implement Exhaustive search for ERM hypothesis\n",
        "\n",
        "Why the algorithm failed? Understanding the root cause is critical. In particular, this means that using a binary search over the threshols is NOT a successful PAC learner.\n",
        "\n",
        "Without strong prior knowledge about the underline data-generation process, we can't apply the same trick of binary bisection (as we did with the first dataset). In most cases, we'll have to check all possible threshold **which are relevant** for the given training data (this was a hint).\n",
        "\n",
        "Putting it differently, now we have to check $\\cal{O}(n)$ thresholds (rather than $\\cal{O}(\\log_2{n})$. In practice, sometimes we check only a random subset of the relevant thresholds, this speed up the running time but decrease the quality of the estimation.\n",
        "\n",
        "Write an exhaustive search for the ERM hypothesis and show that it succeed to find a better predictor on `s1ex2.csv`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87ed514e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:22:34.814917Z",
          "start_time": "2023-10-19T14:22:34.809373Z"
        },
        "id": "87ed514e"
      },
      "outputs": [],
      "source": [
        "# Implement exhaustive search for ERM over thresholds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b49abf8a",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:32:12.665812Z",
          "start_time": "2023-10-19T14:32:07.384808Z"
        },
        "code_folding": [
          27,
          31
        ],
        "scrolled": false,
        "id": "b49abf8a"
      },
      "outputs": [],
      "source": [
        "def threshold_erm_exhaustive(df):\n",
        "    \"\"\" The function should return a callback which accepts a single float and return zero or one \"\"\"\n",
        "    pass\n",
        "\n",
        "exhaustive_erm1 = threshold_erm_exhaustive(df1)\n",
        "\n",
        "print(\n",
        "    'Real threshold:', real_threshold, '\\n',\n",
        "    'empirical error:', training_error(df1, exhaustive_erm1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39eee7be",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:32:07.382285Z",
          "start_time": "2023-10-19T14:32:03.140087Z"
        },
        "id": "39eee7be"
      },
      "outputs": [],
      "source": [
        "exhaustive_erm2 = threshold_erm_exhaustive(df2)\n",
        "\n",
        "print(\n",
        "    'Real threshold:', real_threshold, '\\n',\n",
        "    'empirical error:', training_error(df2, exhaustive_erm2)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44c59cbb",
      "metadata": {
        "id": "44c59cbb"
      },
      "source": [
        "#### Approximation Error\n",
        "\n",
        "Lastly, we will try to use the exhaustive threshold search over a new training set (same prediction task), named `s1ex3.csv`.\n",
        "\n",
        "Find the optimal threshold $\\theta$ and direction $b$, and then compute the empirical risk of that predictor.\n",
        "\n",
        "Hint: it can be useful to generate two plots: the line of the empirical loss of various candidates over the range $[0,1]$, and the data itself (as a scatter plot)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59abc22f",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:41:50.930837Z",
          "start_time": "2023-10-19T14:41:50.917465Z"
        },
        "id": "59abc22f"
      },
      "outputs": [],
      "source": [
        "# Explore the dataset, then run the ERM exhaustive search\n",
        "# to find the best threshold function and compute its empirical risk.\n",
        "df3 = pd.read_csv('s1ex3.csv')\n",
        "df3.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "554c58e4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:40:15.545889Z",
          "start_time": "2023-10-19T14:40:11.465544Z"
        },
        "id": "554c58e4"
      },
      "outputs": [],
      "source": [
        "exhaustive_erm3 = threshold_erm_exhaustive(df3)\n",
        "\n",
        "print(\n",
        "    'empirical error:', training_error(df3, exhaustive_erm2)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ec5bd49",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:42:23.172086Z",
          "start_time": "2023-10-19T14:42:20.706967Z"
        },
        "id": "6ec5bd49"
      },
      "outputs": [],
      "source": [
        "loss3 = calc_stats_for_multiple_thresholds(df3)\n",
        "loss3.plot.line(x='thr', y='total_mistakes')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4558b994",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:42:23.393537Z",
          "start_time": "2023-10-19T14:42:23.174601Z"
        },
        "id": "4558b994"
      },
      "outputs": [],
      "source": [
        "df3.sample(n=500).plot.scatter(x='x', y='label')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57406afd",
      "metadata": {
        "id": "57406afd"
      },
      "source": [
        "What is the empirical risk of the threshold classifier? What is the inherent limitation of our prediction algorithm?\n",
        "\n",
        "Note that the empirical risk can be lower than 0.05, see in the next cell such a predicor (it also give a hint for the questions in the previous line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d20bc383",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:48:56.018364Z",
          "start_time": "2023-10-19T14:48:55.554772Z"
        },
        "id": "d20bc383"
      },
      "outputs": [],
      "source": [
        "def segments_predictor(x):\n",
        "    if 0.2 <= x <= 0.6:\n",
        "        return 1\n",
        "    if 0.85 <= x:\n",
        "        return 1\n",
        "    return 0\n",
        "\n",
        "print('empirical error:', training_error(df3, segments_predictor))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need a way to split the real-line into multiple (i.e. more than 2) segments. We will get back to this in session 3, where both the segments' boundaries and the number of segments will be derived from the data-itself.\n",
        "\n",
        "Find below a toy algorithm for finding a greedy partition to a given (!) number of segments. You can run it with multiple values for `num_segments` and see the results for yourself."
      ],
      "metadata": {
        "id": "j5CRp9cEfWU9"
      },
      "id": "j5CRp9cEfWU9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd7ce948",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2023-10-19T14:44:41.174763Z",
          "start_time": "2023-10-19T14:44:36.022321Z"
        },
        "id": "bd7ce948"
      },
      "outputs": [],
      "source": [
        "def intervals_split_err(left_counts, right_counts):\n",
        "    # predicting majority within each side, means the total num mistakes is the minorities combined\n",
        "    return min(left_counts) + min(right_counts)\n",
        "\n",
        "def intervals_split_stats(left_counts, right_counts):\n",
        "    def predict_majority_stats(counts):\n",
        "        majority_label = 1 if counts[1] > counts[0] else 0\n",
        "        return {\n",
        "            'label': majority_label,\n",
        "            'size': counts[0] + counts[1],\n",
        "            'mistakes': counts[1 - majority_label]\n",
        "        }\n",
        "\n",
        "    left = predict_majority_stats(left_counts)\n",
        "    right = predict_majority_stats(right_counts)\n",
        "\n",
        "    return left, right\n",
        "\n",
        "def erm_for_segments(df, num_segments, split_err_func, split_stats_func, start=0.0, end=1.0):\n",
        "\n",
        "    def find_segment_to_split(segments):\n",
        "        key, _ = max(segments.items(), key = lambda k_v: k_v[1]['mistakes'])\n",
        "        return key\n",
        "\n",
        "    df_fragment = df\n",
        "\n",
        "    df_stats = to_label_stats(df)\n",
        "    segments = {\n",
        "        (start, end): {'label': df_stats.argmax(), 'size': len(df), 'mistakes': df_stats.min()}\n",
        "    }\n",
        "\n",
        "    while len(segments) < num_segments:\n",
        "        segment_key_to_split = find_segment_to_split(segments)\n",
        "\n",
        "        if segments[segment_key_to_split]['mistakes'] == 0:\n",
        "            break\n",
        "\n",
        "        relevant_df_fragment = df.loc[df['x'].between(*segment_key_to_split)].sort_values('x')\n",
        "        curr_split = suggest_best_split(relevant_df_fragment, split_err_func, split_stats_func)\n",
        "\n",
        "        # new left and right segments:\n",
        "        segments[(segment_key_to_split[0], curr_split['thr'])] = curr_split['left']\n",
        "        segments[(curr_split['thr'], segment_key_to_split[1])] = curr_split['right']\n",
        "\n",
        "\n",
        "        del segments[segment_key_to_split]\n",
        "\n",
        "    return segments\n",
        "\n",
        "\n",
        "res_segmenst = erm_for_segments(df3,\n",
        "                            num_segments=4,\n",
        "                            split_err_func=intervals_split_err,\n",
        "                            split_stats_func=intervals_split_stats)\n",
        "print('result:')\n",
        "pprint(res_segmenst)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vpfZHQ2GfC7g"
      },
      "id": "vpfZHQ2GfC7g",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}